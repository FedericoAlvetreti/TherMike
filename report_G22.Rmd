---
title: "Statistical Learning - Final Project Report"
author: "Alvetreti - Corrias - Dicunta - Di Nino"
date: "2023-09-14"
header-includes:
  - \usepackage{fancyhdr}
  - \usepackage{makeidx}
  - \makeindex
  - \usepackage{lastpage}
  - \usepackage{bbm}
  - \usepackage{mdframed}
  - \usepackage{mathtools}
  - \usepackage{lmodern}
  - \usepackage{amsmath}
  - \pagestyle{fancy}
  - \fancyhead[RO,RE]{\thepage}
  - \fancyhead[LO,LE]{Statistical Learning - Notes from the course}
  - \usepackage{amsmath}
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \DeclareMathOperator*{\argmin}{arg\,min}
output: 
  rmdformats::readthedown:
    theme: cerulean
    highlight: espresso
---

```{r setup, include=FALSE}
library(reticulate)
knitr::opts_chunk$set(echo = TRUE)
```

# TherMike - hearing hot loud

# Data collection and experimental design

# Feature extraction and physical phenomenon

# The statistical framework and some math-behind 

Our problem is clearly a regression one: we have to predict water temperature (our response variable) from the sound it makes when poured. The real complexity in this problem is to deal with the covariates. The background for this model is functional data analysis: in every of our attempts we had to deal with features that are clearly time-dependent or frequency-dependent, or can be considered as discrete sampling from continuos and smooth functions. The main reference we adopted is *Introduction to functional data analysis*, an introductive survey in this field by Piotr Kokoszka and Matthew Reimherr. In particular we focused on chapter four, where scalar-response-on-functional-variable linear regression is described.
Since an assumption for a linear underlying model in such a setting could have been a weak hypotesis, we decided to go a little further and try to inject non-linearity in our model. We had to choose between two main approaches:

+ the *continuously additive model* where the underlying generating model is 
\begin{equation}
Y_i = \alpha + \int f(X_i(t),t)dt + \epsilon_i
\end{equation}

and the functional form $f$ is estimated through a splines expansion;

+ the extension to the functional framework of well known *nonparametric models*. 

We decided to go for the second option since it was interesting to get deeper in the field of nonparametric statistic and to study the assumptions we have to do to generalize these approaches in a functional settings.

The two models we mainly worked on are two well-known nonparametric estimators of the response function $m(x)$ being $Y_i = m(x_i) + \epsilon_i$ and $m(x) = \mathbb{E}[Y|X=x]$:

1) The Nadaraya-Watson kernel estimator;
2) The k-Nearest-Neighbours estimator.

Both of this models are based on a concept of distance that requires few adjustments to be used in a functional framework. We are going to consider for now a single functional covariate to define a general approach in approximating distances between functions: the further generalization of this for vectors of functional covariates meets some other issues. 

Being in a functional space, let's assume $x(t),y(t) \in L^2([0,1])$ and consider a complete orthonormal basis $\mathcal{H} = \{e_j\}_{j=1}^{\infty}$. Under this assumptions Parseval's identity holds.
\begin{equation}
||x(t)||_{L^2}^2 = \sum_{j=1}^{\infty} |\langle x(t),e_j(t) \rangle|^2
\end{equation}

This means that for a distance between function we have the following:
\begin{equation}
||x(t)-y(t)||_{L^2}^2 = \sum_{j=1}^{\infty} |\langle x(t)-y(t),e_j(t) \rangle|^2
\end{equation}

Since the inner product is a bilinear form we have that $\langle x(t)-y(t),e_j(t) \rangle = \langle x(t),e_j(t) \rangle -\langle y(t),e_j(t) \rangle$. This means that we can express the $L^2$-distance between functions as the euclidean distance between the vectors of the projections of the functions on the orthonormal basis:

\begin{gathered}
||x(t)-y(t)||_{L^2}^2 = \sum_{j=1}^{\infty} |\langle x(t)-y(t),e_j(t) \rangle|^2 = \nonumber \\
\sum_{j=1}^{\infty} |\langle x(t),e_j(t) \rangle -\langle y(t),e_j(t) \rangle|^2 =   \nonumber \\
||\underline{\beta}_x^{\infty}-\underline{\beta}_y^{\infty}||_2^2
\end{gathered}

So in the end we can approximate the distance between the two functions through a basis expansions on a certain number $G$ of generators:

\begin{equation}
||x(t)-y(t)||_{L^2}^2 \approx ||\hat{\beta}_x-\hat{\beta}_y||_2^2
\end{equation}

where $\hat{\beta}$ is the vector of the empirical Generalized Fouerier Coefficients gathered from a finite basis expansion on an orthonormal basis: in our case we'll go for a cosine basis. This approach requires a basic remapping of the covariates on the domain $[0,1]$.

## Nadaraya-Watson kernel estimator

The Nadarayan-Watson kernel estimator takes the following form:

\begin{equation}
m(X(t)) = \frac{\sum_{i=1}^N Y_iK(h^{-1}d(X_i(t),X(t)))}{\sum_{j=1}^N K(h^{-1}d(X_j(t),X(t)))}
\end{equation}

Given a new signal $X(t)$ the model predicts the response averaging through the whole dataset building a system of weights based on the distance $d(X_i,X)$, a smoothing kernel $K$ that in our case is the gaussian one and a parameter to be tuned through cross-validation $h$: the size of our datasets allows for a leave one out cross validation routine. 

In a classic setting of a single continuously defined covariate $X$ and a continuosly defined response $Y$ we have the following:

\begin{equation}
\mathbb{E}[Y|X=x] = \int yf(y|x)dy = \int \frac{yf(x,y)}{f(x)}dy
\end{equation}

Through a kernel density estimation with a kernel $K_h$ on both the joint distribution and the marginal on $x$ we have the following estimate for the regression function:

\begin{gather}
\hat{\mathbb{E}}[Y|X=x] = \int \frac{y\hat{f}(x,y)}{\hat{f}(x)}dy = \\ \nonumber
\int \frac{y\frac{1}{N}\sum_{j=1}^{N} K_h(d(x,x_j))K_h(d(y,y_j))}{\frac{1}{N}\sum_{j=1}^{N} K_h(d(x,x_j))}dy = \\ \nonumber
\frac{\sum_{j=1}^{N} K_h(d(x,x_j))\int y_jK_h(d(y,y_j))dy}{\sum_{j=1}^{N} K_h(d(x,x_j))} = \\ \nonumber
\frac{\sum_{j=1}^{N} K_h(d(x,x_j))y_j}{\sum_{j=1}^{N} K_h(d(x,x_j))}
\end{gather}

So our estimator has the same structure and derivation as the one above expect for the definition of distance involved.

## k-Nearest-Neighbours regression

In this case the estimate for a regression function is just a local average of the response of the $K$ nearest data points: again in our case it is as the same as the classic formulation, except for the definition of distance.

\begin{equation}
\hat{m}(x) = \frac{1}{|K_{nn}|} \sum_{k \in K_{nn}} Y_k
\end{equation}

Again the parameter $K$ can be validate through a LOOCV routine. 

# Implementation - the failures

## Mel Filters Cepstrum Coefficients: from a single functional covariate to multiple functional covariates

One of the standard approaches when dealing with audio processing in machine learning especially in classification are Mel Filters Cepstrum Coefficients (MFCC). This feature extraction pipeline resambles the behaviour of the evolution of the spectrogram, and the global effect is to gather from an audio the information that mostly are related with the way our ears actually hear. 

So we implemented with simple Python scripts an extraction pipeline working on the benchmark dataset leveraging ```librosa``` modules: 

```{python,eval=F}
import os 
import tqdm
import librosa
import numpy as np
from matplotlib import pyplot as plt
import skfda
import math 

paper_files = os.listdir("./processed-recs-paper")

dic_2 = {file:{'Label':None,
               'MFCC':None} for file in paper_files}
               
for file in tqdm.tqdm(paper_files):
    path = './processed-recs-paper/' + file
    audio, src = librosa.load(path)
    dic_2[file]['MFCC'] = librosa.feature.mfcc(y=audio, 
                                               sr=src, 
                                               n_mfcc=20, 
                                               dct_type=2,
                                               n_fft=1012, 
                                               hop_length=256,
                                               norm='ortho')
                                               
    dic_2[file]['Label'] = float(file.split('_')[3][:-4])
```

The point was that now we did not have a single covariate for each audio; now we have for each audio a vector of functional data, well represented by a matrix having a number of rows equal to the number of filters and a variable number of columns equal to the output of the extraction pipeline. 
So first of all we leveraged ```sklearn.fda``` modules to project this data on a cosine basis after a remapping such that $ t \in [0,1]$ in order to use the previous consideration we made on the approximation for distances between functions:

```{python, eval=F}
# Functional tool kit

# Generate a Fourier basis for the representation

basis = skfda.representation.basis.Fourier(n_basis=40)

# This routine generates a FDataGrid object

def FDA_generator(array):
    # Rearranging the points in [0,1]
    points = np.linspace(0,1,len(array))    
    
    # Generating the FDataGrid
    fd_obj = skfda.FDataGrid(data_matrix=[array],     
                             grid_points=points)
    return fd_obj

# First transform pipeline: retrieve eGFC from the extracted features

def FDAprocessing(matrix, basis):
    output = np.zeros((np.shape(matrix)[0],basis.n_basis))
    
    # The process is done for each of the filter's output, so for each row of the matrix
    for i in range(0,np.shape(matrix)[0]):
      
        # Subroutine call
        fda_obj = FDA_generator(matrix[i,])
        
        # We only retrieve the coefficients of the expansion
        output[i,:] = fda_obj.to_basis(basis).coefficients
    return output

# Empty tensor to store the results of the pipeline

design_tensor = np.zeros((20,41,len(dic_2.values())))

# Main extraction loop
for i in tqdm.tqdm(range(len(dic_2.keys()))):
    key = list(dic_2.keys())[i]
    design_tensor[:,:,i] = FDAprocessing(dic_2[key]['MFCC'],basis)

# Retrieving ground truth labels

y_true = np.array([dic_2[key]['Label'] for key in dic_2.keys()])
```

Now the question was how to generalize the consideration we made about distances between functions now that every audio is represented by the following matrix: 

\begin{pmatrix} 
\text{___}f_1(t)\text{___} \\ 
\text{___}f_2(t)\text{___} \\ 
\vdots \\ 
\text{___}f_p(t)\text{___} 
\end{pmatrix}

being $p=20$ the number of Mel filters deployed and being every row the collection of the empirical GFC for that filter. So we now have for each of the j-th audio signal a matrix $F_j$: the intuition was to consider the distance between each of the related components, gathering the following vector: 

\begin{equation}
d(F_i,F_k) = \begin{pmatrix} 
d(f_{1i}(t),f_{1k}(t)) \\ 
d(f_{2i}(t),f_{2k}(t)) \\ 
\vdots \\ 
d(f_{pi}(t),f_{pk}(t))
\end{pmatrix}
\end{equation}

The problem still was how to plug-in this vector of "component-wise" distance in our distance-based estimator. We came across the article *Nonparametric regression and classification with functional, categorical, and mixed covariates*, by Leonie Selk and Jan Gertheiss, from *Advances in Data Analysis and Classification* (2023). The idea is to leverage the properties of an exponential multidimensional kernel estimator to be rewritten as follow: 

\begin{equation}
\hat{m}(x) = \frac{\sum_{i=1}^N Y_iK(\omega_1 d_1(X_{i1},x_{1})) + ... + \omega_p d_p(X_{ip},x_p))}{\sum_{i=1}^K(\omega_1 d_1(X_{i1},x_{1})) + ... + \omega_p d_p(X_{ip},x_p))}
\end{equation}

where $x = (x_1, ..., x_p)^T$ is the vector of miscellaneous covariates. The power of the proposed framework is that we can implement this for a wide mixture of typologies of variables, up to the requirements that we must have a definition of distance for each of those, so this apply to our multiple functional covariates scenario. 

Given the kernel estimator, we can write the associated leave one out cross validation error as 

\begin{equation}
\hat{Err}(\omega_1,...,\omega_p) = \sum_{i=1}^N (Y_i - \hat{Y}_{(-i)})^2
\end{equation}

and define the following optimization problem:

\begin{cases}
\underset{\omega_1,...,\omega_p}{\mathrm{min}} \hat{Err} \\
\omega_i \geq 0 & i = 1,...,p
\end{cases}

In a way this becomes a sort of semiparametric approach to the problem, or an extended validation procedure where we are forcing optimality rather than do a classic grid search. 

The following Python code defines the routine needed to implement this optimization problem to be solved by ```scipy.optimize```'s module ```minimize```.

```{python, eval=F}
# Non-parametric regression on a vectorial functional space 

# Gaussian kernel
def K(t):
    return(0.5*np.exp(-0.5*(t**2)))

# L2 distance
def L2(x1,x2):
    return np.linalg.norm(x1-x2)

# Weighted distance between the rows of the matrices
def weightedCompWiseDist(X1,X2,omega):
    L = np.shape(X1)[0]
    D = np.zeros(L)

    for i in range(0,L):
        D[i] = L2(X1[i,:],X2[i,:])
    
    return np.sum(D*omega)

# Kernel regression estimator
def KR_estimator(x,X,Y,omega):
    weights = np.ones(len(Y))
    for i in range(len(Y)):
        weights[i] = K(weightedCompWiseDist(x,X[:,:,i],omega))
    return np.sum(weights*Y)/np.sum(weights)  

# Minimization problem objective function 
def objective(params):
    omega = params[0:20]
    output = 0
    for i in range(len(y_true)):
        # Manually implementing LOOCV
        
        # i-th data-point
        x = design_tensor[:,:,i]
        
        # i-th true response
        y = y_true[i]
        
        # Dataset {X,Y} dropping the i-th sample
        _X = design_tensor[:,:,[j for j in range(0,len(y_true)) if j != i]]
        _Y = y_true[[j for j in range(0,len(y_true)) if j != i]]
        
        # Making a kernel regression prediction
        pred = KR_estimator(x,_X,_Y,omega)
        
        # Increasing the global error
        output += (pred-y)**2
    
    # Retrieving the mean square error in squared root
    return np.sqrt(output/len(y_true))

```

The result was quiet weak despite our good intentions:

```{r,echo=F}
knitr::include_graphics("immagine/img1.png")
```

We could only achieve values for the RMSE of around 19, that is far worse than our best results. This could be due to several reasons: 

+ The audio needed more pre-processing in order to gather finer features when applying MFCC;

+ Consequently, the coefficients of the basis expansion shows non regular behaviours;

+ The optimization routine lacks in precision because of the initialization: a local search could have been a good improvement for a correct initialization, and maybe further optimization strategies like sequential or block decomposition ones could have improved the result;

By the way this implementation was very formative, especially because this forced us to learn about MFCC that was the basis for our personalized feature extraction. 

## Bagging cross validation for the bandwidth 
Before we realized that the dimension of our dataset perfectly allows for a full LOOCV pipeline, we were wondering about a fancy validation procedure to complete the state-of-art formation we gathered through the homeworks (nested cross validation, LOCO). We decided to implement bagging cross validation from the bandwidth in Nadaraya-Watson estimator as it is described in *Bagging cross-validated bandwidth selection in nonparametric regression estimation with applications to large-sized samples*, where the idea is actually very simple. In large dataset we can perform LOOCV for optimization on repeated subsamples of the data and then average the optimal value on each of the subsample. The optimal sample size minimizing the variance of this estimate is $r = \frac{n}{3\sqrt{N}}$, being $n$ the dataset size and $N$ the number of repeated subsampling. This expression is expressive enough to conclude that for small datasets this is completely unuseful and makes the whole procedure instable, but it was helpful since made us realize that in our case LOOCV was not so computationally expensive. 


# Implementation - the successes

# Result and comments 
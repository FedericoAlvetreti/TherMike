---
title: "Statistical Learning - Final Project Report"
author: "Alvetreti - Corrias - Dicunta - Di Nino"
date: "2023-09-14"
bibliography: bibliography.bib

header-includes:
  - \usepackage{fancyhdr}
  - \usepackage{makeidx}
  - \makeindex
  - \usepackage{lastpage}
  - \usepackage{bbm}
  - \usepackage{mdframed}
  - \usepackage{mathtools}
  - \usepackage{lmodern}
  - \usepackage{amsmath}
  - \pagestyle{fancy}
  - \fancyhead[RO,RE]{\thepage}
  - \fancyhead[LO,LE]{Statistical Learning - Notes from the course}
  - \usepackage{amsmath}
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \DeclareMathOperator*{\argmin}{arg\,min}
output: 
  rmdformats::readthedown:
    theme: cerulean
    highlight: espresso
editor_options: 
  markdown: 
    wrap: sentence
---

```{r, include=FALSE}
knitr::write_bib(x = c("knitr", "rmarkdown") , file = "test.bib")
```


```{r setup, include=FALSE}
library(reticulate)
knitr::opts_chunk$set(echo = TRUE)
```

# TherMike - hearing hot loud

# Data collection and experimental design

The dataset we used deals with 642 audio signals,  each representing the sound produced while pouring water at a certain temperature (label). To reach a valid number of samples for the regression problem we both took manually our data and retrieved 333 tracks from the paper [@Thermophone]. The setup for the measurement was made in a way to ensure **reproducibility** for all the members of the group.

In particular, an **online application** was written to get the audio with the same digital characteristics such as the compression algorithm, the sampling rate and so on.  

<center>

![**Fig. 1**. Streamlit application ](IMG\\app1.png){width=40%}

</center>


Notice we added to our original dataset another variable containing the information on the person taking that particular measurement. In this way it could be possible to eventually use it as feature to better classify the temperature. Unfortunately, with the feature extraction we implemented this information turned out to be useless.  


The application served also as **centralized** collector of data that allowed us to keep track of all the measurements at any time. 

In the end, the labels resulted quite balanced, with a greater number of related observations at the middle temperatures, that is, in the range [30$^\circ$, 60$^\circ$]

<center>

![**Fig. 2**. Histogram of data](IMG\\data_hist.png){width=50%}

</center>

The standards of the experimental measurements were fixed such that we could use everyday objects. In particular, we used: 

- two metallic containers, one for pouring, on to pour on. The distance between the former and the latter was set at 10 cm; 
- a food thermometer having precision $\pm 0.5^\circ$;
- our amazing app by which we could use any device equipped with a microphone. 

They differ from the measurements from the paper in the material of the containers, the accuracy of the thermometer (there $\pm 1^\circ$) and the microphone which, in their case, was specifically a microphone from a computer headset. These observation were used as benchmark for the first attempts with our model. 

# Feature extraction and physical phenomenon

In order to understand *a priori* how the algorithm will output the predicted temperature, we thought a physical explanation of the phenomenon could be useful.
In particular, since we start from the power spectrum, the vibrational phenomena are particularly significant.
Let's start from a premise: it makes sense to think you can determine the water temperature by the sound it makes while poured [@Thermophone].
This is true both for empirical evidence (even human ear could detect a difference) both for the change in viscosity and density due to a variation of the temperature.
We see that both of them decrease as the temperature increases.


<center>

![**Fig. 3**. Density and Viscosity vs temperature](IMG\\density_viscosity.png){width=50%}

</center>

The intuition is to analyse the sound made by pouring water in a way that the algorithm can classify from it.
The approach is to study the intensities of different **frequencies** making up the sound.
Notice that these frequencies are the acoustic emissions formed by air bubbles when the water surface causes air to be trapped in it.
After formation, these bubbles emit a *sinusoidal sound* which decays as energy is dissipated.

Hence, we could think that the harmonic content is different for cold and hot water.
Unfortunately, by looking at the literature [@Psycoacoustic] it turns out that there there is not such a remarkable difference in the values of the frequencies and that the trends of the frequency lines are similar at different temperatures.



<center>

![**Fig. 4**. Hot and Cold water pouring spectrograms](IMG\HOTvsCOLD_spectograms.png){width=50%}

</center>

So, how is it possible to classify?
Some differences in the harmonic content must be there, otherwise no model would be effective.
We could start from a result based on the distinction between the three different sources of vibration: the container, the water sound and the resonance of the air column in the container, that is, the fundamental resonant frequency at which the wave displacement is at its maximum.
Apparently, these sources differ in intensity for cold and hot water.
In particular, the sound of the vibration of the container and the bubbles in the liquid take a dominant role in the sound of pouring **cold water** and the sound o resonance of the air column is more relevant in **hot water**.

In an intuitive way, this can be explained by the fact that the highest frequencies in both hot and cold water sounds are justified in the different case by two different phenomena: there tends to be more bubbling in a liquid that's hot meaning that there are higher frequencies coming from it.
On the other way cold water is more viscous, causing high pitched ringing.

Hence, we decided to not use the simple power spectrum in the pipeline but to extract features from it.

Since the distinction lies in the sources of vibration, we thought a good choice for the feature extraction could be to rely on low-level features summarizing what in music is called *timbre*, or tone color.
In the literature [@timbre] many attempts have been made to extract timbre features.
We decided to design our own method starting from an intuition: the *human ear* can perceive a difference in hot and cold water, no matter how nuanced, by the sound of pouring.
By means of the feature extraction we wanted to capture what the human ear does when listening and boost the mechanism to make it more accurate.
The mechanism consists of two fundamental steps:

1.  filter the power spectrum with a Mel Filterbank to return an estimate of the perceived pitch;
2.  apply Weber-Fechner law of psychophysics that models the intensity of the perception as a logarithm.

The results are vectors of 200 features representing for each window of the filterbank the logarithmic mean of the power spectrum.
Just to visualize, we can show a plot of the features for very hot (\>80 $^\circ$) and very cold (\<20 $^\circ$) water sounds.


<center>

![**Fig. 5**. Extracted features for hot and cold water](IMG\\features3.png){width=90%}

</center>
We can see a difference between them, especially for the first 100 features.
The model should be able to discriminate among the different audio signals.
By the first results it turns out that the algorithm, by using the features extracted from the power spectrum, can output roughly the right temperature with a difference degree of correctness depending on the range of temperatures we are considering.

# The statistical framework and some math-behind

Our problem is clearly a regression one: we have to predict water temperature (our response variable) from the sound it makes when poured.
The real complexity in this problem is to deal with the covariates.
The background for this model is functional data analysis: in every of our attempts we had to deal with features that are clearly time-dependent or frequency-dependent, or can be considered as discrete sampling from continuos and smooth functions.
The main reference we adopted is *Introduction to functional data analysis*, an introductive survey in this field by Piotr Kokoszka and Matthew Reimherr.
In particular we focused on chapter four, where scalar-response-on-functional-variable linear regression is described.
Since an assumption for a linear underlying model in such a setting could have been a weak hypotesis, we decided to go a little further and try to inject non-linearity in our model.
We had to choose between two main approaches:

-   the *continuously additive model* where the underlying generating model is \begin{equation}
    Y_i = \alpha + \int f(X_i(t),t)dt + \epsilon_i
    \end{equation}

and the functional form $f$ is estimated through a splines expansion;

-   the extension to the functional framework of well known *nonparametric models*.

We decided to go for the second option since it was interesting to get deeper in the field of nonparametric statistic and to study the assumptions we have to do to generalize these approaches in a functional settings.

The two models we mainly worked on are two well-known nonparametric estimators of the response function $m(x)$ being $Y_i = m(x_i) + \epsilon_i$ and $m(x) = \mathbb{E}[Y|X=x]$:

1)  The Nadaraya-Watson kernel estimator;
2)  The k-Nearest-Neighbours estimator.

Both of this models are based on a concept of distance that requires few adjustments to be used in a functional framework.
We are going to consider for now a single functional covariate to define a general approach in approximating distances between functions: the further generalization of this for vectors of functional covariates meets some other issues.

Being in a functional space, let's assume $x(t),y(t) \in L^2([0,1])$ and consider a complete orthonormal basis $\mathcal{H} = \{e_j\}_{j=1}^{\infty}$.
Under this assumptions Parseval's identity holds.
\begin{equation}
||x(t)||_{L^2}^2 = \sum_{j=1}^{\infty} |\langle x(t),e_j(t) \rangle|^2
\end{equation}

This means that for a distance between function we have the following: \begin{equation}
||x(t)-y(t)||_{L^2}^2 = \sum_{j=1}^{\infty} |\langle x(t)-y(t),e_j(t) \rangle|^2
\end{equation}

Since the inner product is a bilinear form we have that $\langle x(t)-y(t),e_j(t) \rangle = \langle x(t),e_j(t) \rangle -\langle y(t),e_j(t) \rangle$.
This means that we can express the $L^2$-distance between functions as the euclidean distance between the vectors of the projections of the functions on the orthonormal basis:

```{=tex}
\begin{gathered}
||x(t)-y(t)||_{L^2}^2 = \sum_{j=1}^{\infty} |\langle x(t)-y(t),e_j(t) \rangle|^2 = \nonumber \\
\sum_{j=1}^{\infty} |\langle x(t),e_j(t) \rangle -\langle y(t),e_j(t) \rangle|^2 =   \nonumber \\
||\underline{\beta}_x^{\infty}-\underline{\beta}_y^{\infty}||_2^2
\end{gathered}
```
So in the end we can approximate the distance between the two functions through a basis expansions on a certain number $G$ of generators:

```{=tex}
\begin{equation}
||x(t)-y(t)||_{L^2}^2 \approx ||\hat{\beta}_x-\hat{\beta}_y||_2^2
\end{equation}
```
where $\hat{\beta}$ is the vector of the empirical Generalized Fouerier Coefficients gathered from a finite basis expansion on an orthonormal basis: in our case we'll go for a cosine basis.
This approach requires a basic remapping of the covariates on the domain $[0,1]$.

## Nadaraya-Watson kernel estimator

The Nadarayan-Watson kernel estimator takes the following form:

```{=tex}
\begin{equation}
m(X(t)) = \frac{\sum_{i=1}^N Y_iK(h^{-1}d(X_i(t),X(t)))}{\sum_{j=1}^N K(h^{-1}d(X_j(t),X(t)))}
\end{equation}
```
Given a new signal $X(t)$ the model predicts the response averaging through the whole dataset building a system of weights based on the distance $d(X_i,X)$, a smoothing kernel $K$ that in our case is the gaussian one and a parameter to be tuned through cross-validation $h$: the size of our datasets allows for a leave one out cross validation routine.

In a classic setting of a single continuously defined covariate $X$ and a continuosly defined response $Y$ we have the following:

```{=tex}
\begin{equation}
\mathbb{E}[Y|X=x] = \int yf(y|x)dy = \int \frac{yf(x,y)}{f(x)}dy
\end{equation}
```
Through a kernel density estimation with a kernel $K_h$ on both the joint distribution and the marginal on $x$ we have the following estimate for the regression function:

```{=tex}
\begin{gather}
\hat{\mathbb{E}}[Y|X=x] = \int \frac{y\hat{f}(x,y)}{\hat{f}(x)}dy = \\ \nonumber
\int \frac{y\frac{1}{N}\sum_{j=1}^{N} K_h(d(x,x_j))K_h(d(y,y_j))}{\frac{1}{N}\sum_{j=1}^{N} K_h(d(x,x_j))}dy = \\ \nonumber
\frac{\sum_{j=1}^{N} K_h(d(x,x_j))\int y_jK_h(d(y,y_j))dy}{\sum_{j=1}^{N} K_h(d(x,x_j))} = \\ \nonumber
\frac{\sum_{j=1}^{N} K_h(d(x,x_j))y_j}{\sum_{j=1}^{N} K_h(d(x,x_j))}
\end{gather}
```
So our estimator has the same structure and derivation as the one above expect for the definition of distance involved.

## k-Nearest-Neighbours regression

In this case the estimate for a regression function is just a local average of the response of the $K$ nearest data points: again in our case it is as the same as the classic formulation, except for the definition of distance.

```{=tex}
\begin{equation}
\hat{m}(x) = \frac{1}{|K_{nn}|} \sum_{k \in K_{nn}} Y_k
\end{equation}
```
Again the parameter $K$ can be validate through a LOOCV routine.

# Implementation - the failures

## Mel Filters Cepstrum Coefficients: from a single functional covariate to multiple functional covariates

One of the standard approaches when dealing with audio processing in machine learning especially in classification are Mel Filters Cepstrum Coefficients (MFCC).
This feature extraction pipeline resambles the behaviour of the evolution of the spectrogram, and the global effect is to gather from an audio the information that mostly are related with the way our ears actually hear.

So we implemented with simple Python scripts an extraction pipeline working on the benchmark dataset leveraging `librosa` modules:

```{python,eval=F}
import os 
import tqdm
import librosa
import numpy as np
from matplotlib import pyplot as plt
import skfda
import math 

paper_files = os.listdir("./processed-recs-paper")

dic_2 = {file:{'Label':None,
               'MFCC':None} for file in paper_files}
               
for file in tqdm.tqdm(paper_files):
    path = './processed-recs-paper/' + file
    audio, src = librosa.load(path)
    dic_2[file]['MFCC'] = librosa.feature.mfcc(y=audio, 
                                               sr=src, 
                                               n_mfcc=20, 
                                               dct_type=2,
                                               n_fft=1012, 
                                               hop_length=256,
                                               norm='ortho')
                                               
    dic_2[file]['Label'] = float(file.split('_')[3][:-4])
```

The point was that now we did not have a single covariate for each audio; now we have for each audio a vector of functional data, well represented by a matrix having a number of rows equal to the number of filters and a variable number of columns equal to the output of the extraction pipeline.
So first of all we leveraged `sklearn.fda` modules to project this data on a cosine basis after a remapping such that \$ t \in [0,1]\$ in order to use the previous consideration we made on the approximation for distances between functions:

```{python, eval=F}
# Functional tool kit

# Generate a Fourier basis for the representation

basis = skfda.representation.basis.Fourier(n_basis=40)

# This routine generates a FDataGrid object

def FDA_generator(array):
    # Rearranging the points in [0,1]
    points = np.linspace(0,1,len(array))    
    
    # Generating the FDataGrid
    fd_obj = skfda.FDataGrid(data_matrix=[array],     
                             grid_points=points)
    return fd_obj

# First transform pipeline: retrieve eGFC from the extracted features

def FDAprocessing(matrix, basis):
    output = np.zeros((np.shape(matrix)[0],basis.n_basis))
    
    # The process is done for each of the filter's output, so for each row of the matrix
    for i in range(0,np.shape(matrix)[0]):
      
        # Subroutine call
        fda_obj = FDA_generator(matrix[i,])
        
        # We only retrieve the coefficients of the expansion
        output[i,:] = fda_obj.to_basis(basis).coefficients
    return output

# Empty tensor to store the results of the pipeline

design_tensor = np.zeros((20,41,len(dic_2.values())))

# Main extraction loop
for i in tqdm.tqdm(range(len(dic_2.keys()))):
    key = list(dic_2.keys())[i]
    design_tensor[:,:,i] = FDAprocessing(dic_2[key]['MFCC'],basis)

# Retrieving ground truth labels

y_true = np.array([dic_2[key]['Label'] for key in dic_2.keys()])
```

Now the question was how to generalize the consideration we made about distances between functions now that every audio is represented by the following matrix:

```{=tex}
\begin{pmatrix} 
\text{___}f_1(t)\text{___} \\ 
\text{___}f_2(t)\text{___} \\ 
\vdots \\ 
\text{___}f_p(t)\text{___} 
\end{pmatrix}
```
being $p=20$ the number of Mel filters deployed and being every row the collection of the empirical GFC for that filter.
So we now have for each of the j-th audio signal a matrix $F_j$: the intuition was to consider the distance between each of the related components, gathering the following vector:

```{=tex}
\begin{equation}
d(F_i,F_k) = \begin{pmatrix} 
d(f_{1i}(t),f_{1k}(t)) \\ 
d(f_{2i}(t),f_{2k}(t)) \\ 
\vdots \\ 
d(f_{pi}(t),f_{pk}(t))
\end{pmatrix}
\end{equation}
```
The problem still was how to plug-in this vector of "component-wise" distance in our distance-based estimator.
We came across the article *Nonparametric regression and classification with functional, categorical, and mixed covariates*, by Leonie Selk and Jan Gertheiss, from *Advances in Data Analysis and Classification* (2023).
The idea is to leverage the properties of an exponential multidimensional kernel estimator to be rewritten as follow:

```{=tex}
\begin{equation}
\hat{m}(x) = \frac{\sum_{i=1}^N Y_iK(\omega_1 d_1(X_{i1},x_{1})) + ... + \omega_p d_p(X_{ip},x_p))}{\sum_{i=1}^K(\omega_1 d_1(X_{i1},x_{1})) + ... + \omega_p d_p(X_{ip},x_p))}
\end{equation}
```
where $x = (x_1, ..., x_p)^T$ is the vector of miscellaneous covariates.
The power of the proposed framework is that we can implement this for a wide mixture of typologies of variables, up to the requirements that we must have a definition of distance for each of those, so this apply to our multiple functional covariates scenario.

Given the kernel estimator, we can write the associated leave one out cross validation error as

```{=tex}
\begin{equation}
\hat{Err}(\omega_1,...,\omega_p) = \sum_{i=1}^N (Y_i - \hat{Y}_{(-i)})^2
\end{equation}
```
and define the following optimization problem:

```{=tex}
\begin{cases}
\underset{\omega_1,...,\omega_p}{\mathrm{min}} \hat{Err} \\
\omega_i \geq 0 & i = 1,...,p
\end{cases}
```
In a way this becomes a sort of semiparametric approach to the problem, or an extended validation procedure where we are forcing optimality rather than do a classic grid search.

The following Python code defines the routine needed to implement this optimization problem to be solved by `scipy.optimize`'s module `minimize`.

```{python, eval=F}
# Non-parametric regression on a vectorial functional space 

# Gaussian kernel
def K(t):
    return(0.5*np.exp(-0.5*(t**2)))

# L2 distance
def L2(x1,x2):
    return np.linalg.norm(x1-x2)

# Weighted distance between the rows of the matrices
def weightedCompWiseDist(X1,X2,omega):
    L = np.shape(X1)[0]
    D = np.zeros(L)

    for i in range(0,L):
        D[i] = L2(X1[i,:],X2[i,:])
    
    return np.sum(D*omega)

# Kernel regression estimator
def KR_estimator(x,X,Y,omega):
    weights = np.ones(len(Y))
    for i in range(len(Y)):
        weights[i] = K(weightedCompWiseDist(x,X[:,:,i],omega))
    return np.sum(weights*Y)/np.sum(weights)  

# Minimization problem objective function 
def objective(params):
    omega = params[0:20]
    output = 0
    for i in range(len(y_true)):
        # Manually implementing LOOCV
        
        # i-th data-point
        x = design_tensor[:,:,i]
        
        # i-th true response
        y = y_true[i]
        
        # Dataset {X,Y} dropping the i-th sample
        _X = design_tensor[:,:,[j for j in range(0,len(y_true)) if j != i]]
        _Y = y_true[[j for j in range(0,len(y_true)) if j != i]]
        
        # Making a kernel regression prediction
        pred = KR_estimator(x,_X,_Y,omega)
        
        # Increasing the global error
        output += (pred-y)**2
    
    # Retrieving the mean square error in squared root
    return np.sqrt(output/len(y_true))

```

The result was quiet weak despite our good intentions:

```{r,echo=F}
knitr::include_graphics("IMG\\img1.png")
```


<center>

![**Fig. 6**. Output of the minimizer](IMG\\img1.png){width=50%}

</center>

We could only achieve values for the RMSE of around 19, that is far worse than our best results.
This could be due to several reasons:

-   The audio needed more pre-processing in order to gather finer features when applying MFCC;

-   Consequently, the coefficients of the basis expansion shows non regular behaviours;

-   The optimization routine lacks in precision because of the initialization: a local search could have been a good improvement for a correct initialization, and maybe further optimization strategies like sequential or block decomposition ones could have improved the result;

By the way this implementation was very formative, especially because this forced us to learn about MFCC that was the basis for our personalized feature extraction.

## Bagging cross validation for the bandwidth

Before we realized that the dimension of our dataset perfectly allows for a full LOOCV pipeline, we were wondering about a fancy validation procedure to complete the state-of-art formation we gathered through the homeworks (nested cross validation, LOCO).
We decided to implement bagging cross validation from the bandwidth in Nadaraya-Watson estimator as it is described in *Bagging cross-validated bandwidth selection in nonparametric regression estimation with applications to large-sized samples*, where the idea is actually very simple.
In large dataset we can perform LOOCV for optimization on repeated subsamples of the data and then average the optimal value on each of the subsample.
The optimal sample size minimizing the variance of this estimate is $r = \frac{n}{3\sqrt{N}}$, being $n$ the dataset size and $N$ the number of repeated subsampling.
This expression is expressive enough to conclude that for small datasets this is completely unuseful and makes the whole procedure instable, but it was helpful since made us realize that in our case LOOCV was not so computationally expensive.

# Implementation - the successes

# Result and comments
